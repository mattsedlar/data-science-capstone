require(tm)
require(R.utils)
require(LaF)
require(SnowballC)

# count lines first to speed up process
twitter.n <- countLines("data/en_US.twitter.txt")
blog.n <- countLines("data/en_US.blogs.txt")
news.n <- countLines("data/en_US.news.txt")

# read lines randomly into a data table column
set.seed(1234)

docs <- c(data.frame(sample_lines("data/en_US.twitter.txt", .0175 * twitter.n,nlines=twitter.n)),
          data.frame(sample_lines("data/en_US.blogs.txt", .0215 * blog.n, nlines=blog.n)),
          data.frame(sample_lines("data/en_US.news.txt", .0215 * news.n, nlines=news.n)))

# # corpora
c <- Corpus(VectorSource(docs))

# check it out
inspect(c)
 
# eliminate whitespace
c <- tm_map(c, stripWhitespace)
(f <- content_transformer(function(x,pattern,sub) gsub(pattern,sub,x)))
# to lower case
c <- tm_map(c, content_transformer(tolower))
# remove numbers
c <- tm_map(c, content_transformer(removeNumbers))

# profanity filtering
bad.words <- c()
bad.nouns <- c("ass","nigg","dick","slut","pussy","cunt","cooch","panties","titt","hell\\b")
bad.verbs <- c("fuck","shit","suck","fart","piss","bitch")

bad.words <- c(bad.words,
               paste(bad.nouns,"(a|as|es|er?)?s?|",sep=""),
               paste(bad.verbs,"(a|e|ed|er|ty|ing?)?s?|",sep=""))

bad.words <- paste(bad.words,collapse="")
bad.words <- substr(bad.words,1,nchar(bad.words)-1)
c <- tm_map(c, f, bad.words,"")

# punctuation
punctuation <- "[^[:alnum:][:space:]']"
c <- tm_map(c, f, punctuation,"")

writeCorpus(c,"data")

# # TOKENIZERS

BigramTokenizer <- function(x) {
 unlist(lapply(ngrams(words(x),2),paste,collapse=" "), use.names = F)
}
TrigramTokenizer <- function(x) {
  unlist(lapply(ngrams(words(x),3),paste,collapse=" "), use.names = F)
}
QuatrogramTokenizer <- function(x) {
  unlist(lapply(ngrams(words(x),4),paste,collapse=" "), use.names = F)
}
#CincogramTokenizer <- function(x) {
#  unlist(lapply(ngrams(words(x),5),paste,collapse=" "), use.names = F)
#}
# 
# # term document matrix for 1-4grams
tdm.1g <- TermDocumentMatrix(c)
tdm.2g <- TermDocumentMatrix(c, control=list(tokenize=BigramTokenizer))
tdm.3g <- TermDocumentMatrix(c, control=list(tokenize=TrigramTokenizer))
tdm.4g <- TermDocumentMatrix(c, control=list(tokenize=QuatrogramTokenizer))
#tdm.5g <- TermDocumentMatrix(c, control=list(tokenize=CincogramTokenizer))
# 
# 
# # finding most-common terms
tdm.1g.matrix <- as.matrix(tdm.1g)
tdm.2g.matrix <- as.matrix(tdm.2g)
tdm.3g.matrix <- as.matrix(tdm.3g)
tdm.4g.matrix <- as.matrix(tdm.4g)
#tdm.5g.matrix <- as.matrix(tdm.5g)

# frequency for calculating probabilities 
frequency.1g <- rowSums(tdm.1g.matrix)
frequency.2g <- rowSums(tdm.2g.matrix)
frequency.3g <- rowSums(tdm.3g.matrix)
frequency.4g <- rowSums(tdm.4g.matrix)
#frequency.5g <- rowSums(tdm.5g.matrix)

# 
frequency.1g <- sort(frequency.1g, decreasing = T)
frequency.2g <- sort(frequency.2g, decreasing = T)
frequency.3g <- sort(frequency.3g, decreasing = T)
frequency.4g <- sort(frequency.4g, decreasing = T)
#frequency.5g <- sort(frequency.5g, decreasing = T)

# create data frames with probabilities

df.1g <- data.frame(token=names(frequency.1g), freq=frequency.1g, prob=frequency.1g/sum(frequency.1g))
df.2g <- data.frame(token=names(frequency.2g), freq=frequency.2g, prob=frequency.2g/sum(frequency.2g))
df.3g <- data.frame(token=names(frequency.3g), freq=frequency.3g, prob=frequency.3g/sum(frequency.3g))
df.4g <- data.frame(token=names(frequency.4g), freq=frequency.4g, prob=frequency.4g/sum(frequency.4g))
#df.5g <- data.frame(token=names(frequency.5g), freq=frequency.5g, prob=frequency.5g/sum(frequency.5g))

write.csv(df.1g,"data/final/onegrams.csv",row.names = F)
write.csv(df.2g,"data/final/twograms.csv",row.names = F)
write.csv(df.3g,"data/final/threegrams.csv",row.names = F)
write.csv(df.4g,"data/final/fourgrams.csv",row.names = F)
#write.csv(df.5g,"data/final/fivegrams.csv",row.names = F)
